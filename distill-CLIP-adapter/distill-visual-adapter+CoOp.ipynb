{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777dd329",
   "metadata": {
    "id": "777dd329"
   },
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_HTZmlp7OSW",
   "metadata": {
    "id": "X_HTZmlp7OSW"
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76baca5d",
   "metadata": {
    "id": "76baca5d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.flowers102 import Flowers102\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import clip\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "from itertools import chain\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TerwRUeLgZoX",
   "metadata": {
    "id": "TerwRUeLgZoX"
   },
   "source": [
    "# Custom Dataset (original image paired with corresponding foreground extracted image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mrXW-tLbgZNm",
   "metadata": {
    "id": "mrXW-tLbgZNm"
   },
   "outputs": [],
   "source": [
    "class PairedFlowers102(Dataset):\n",
    "  \"\"\"\n",
    "  This class enriches the original Flowers102 dataset class with a paired foreground image.\n",
    "  \"\"\"\n",
    "  def __init__(self, root, split, transform=None, fg_transform=None):\n",
    "    self.original_dataset = Flowers102(root=root, split=split, download=True)\n",
    "    self.root = root\n",
    "    self.split = split\n",
    "    self.transform = transform\n",
    "    self.fg_transform = fg_transform or transform\n",
    "\n",
    "    self.fg_dir = os.path.join(self.root, 'foreground')\n",
    "    assert os.path.exists(self.fg_dir), f\"Foreground images not found in {self.fg_dir}\"\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.original_dataset)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    origin_img, label = self.original_dataset[idx]\n",
    "    origin_label = self.original_dataset[idx][1]\n",
    "\n",
    "    # Load corresponding foreground image\n",
    "    filename = os.path.basename(self.original_dataset._image_files[idx])\n",
    "    fg_path = os.path.join(self.fg_dir, filename)\n",
    "    fg_img = Image.open(fg_path).convert('RGB')\n",
    "\n",
    "    # apply image transforms\n",
    "    if self.transform:\n",
    "        origin_img = self.transform(origin_img)\n",
    "    if self.fg_transform:\n",
    "        fg_img = self.fg_transform(fg_img)\n",
    "\n",
    "    return {\n",
    "        \"img_orig\": origin_img,\n",
    "        \"img_fg\": fg_img,\n",
    "        \"label\": label,\n",
    "        \"idx\": idx\n",
    "    }\n",
    "\n",
    "  @property\n",
    "  def _labels(self):\n",
    "    return self.original_dataset._labels\n",
    "\n",
    "  @property\n",
    "  def _image_files(self):\n",
    "    return self.original_dataset._image_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b87a1",
   "metadata": {
    "id": "736b87a1"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8de9a",
   "metadata": {
    "id": "53e8de9a"
   },
   "outputs": [],
   "source": [
    "def base_novel_categories(dataset) -> tuple[list[int], list[int]]:\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes // 2]\n",
    "    novel_classes = list(range(num_classes))[num_classes // 2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "\n",
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "\n",
    "def get_data(data_dir='./data', transform=None) -> tuple[Flowers102,\n",
    "                                                         Flowers102,\n",
    "                                                         Flowers102]:\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = PairedFlowers102(\n",
    "        root=data_dir, split=\"train\", transform=transform, fg_transform=transform)\n",
    "    val = Flowers102(\n",
    "        root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = Flowers102(\n",
    "        root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset\n",
    "\n",
    "\n",
    "def accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    base_indices,\n",
    "    alpha=0.5,\n",
    "    gamma=0.2,\n",
    "    temperature=2.0\n",
    "):\n",
    "    teacher_model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    for batch in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        orig_imgs = batch[\"img_orig\"].to(DEVICE)  # original images\n",
    "        # fg_imgs = batch[\"img_fg\"].to(DEVICE)  # corresponding foreground images\n",
    "        labels = batch[\"label\"].to(DEVICE)\n",
    "\n",
    "        # student forward pass\n",
    "        student_img_feats, student_text_feats = student_model(orig_imgs)  # both with shape: [B, 102]\n",
    "        # print(\"student text_features shape: \", student_text_feats.shape)\n",
    "\n",
    "        # teacher forward pass\n",
    "        with torch.no_grad():\n",
    "            dtype = teacher_model.dtype\n",
    "            teacher_img_feats = teacher_model.visual(orig_imgs.type(dtype))\n",
    "            teacher_img_feats /= teacher_img_feats.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            classnames = student_model.classnames\n",
    "            template = \"a photo of a {}, a type of flower.\"\n",
    "            prompts = [template.format(c) for c in classnames]\n",
    "            tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(DEVICE)\n",
    "            teacher_text_feats = teacher_model.encode_text(tokenized_prompts)\n",
    "            teacher_text_feats /= teacher_text_feats.norm(dim=-1, keepdim=True)\n",
    "            # print(\"teacher text_features shape: \", teacher_text_feats.shape)\n",
    "\n",
    "        # calculate student and teacher logits\n",
    "        logit_scale = student_model.logit_scale.exp()\n",
    "        student_logits = logit_scale * student_img_feats @ student_text_feats.t()\n",
    "        # note: vision logits cos. sim between teacher img feats and student (adapted with CoOp) text feats.\n",
    "        teacher_vision_logits = logit_scale * teacher_img_feats @ student_text_feats.t()\n",
    "\n",
    "        student_logits_base = student_logits[:, base_indices]  # logtis for base classes only\n",
    "\n",
    "        # CE loss on base classes only\n",
    "        ce_loss = F.cross_entropy(student_logits_base, labels)\n",
    "\n",
    "        # visual distillation loss (KL div.) with temperature\n",
    "        vision_kd_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / temperature, dim=-1),\n",
    "            F.softmax(teacher_vision_logits / temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (temperature ** 2)\n",
    "\n",
    "        # text feature-based KD loss\n",
    "        text_kd_loss = 1 - F.cosine_similarity(\n",
    "            student_text_feats, teacher_text_feats, dim=-1\n",
    "        ).mean()\n",
    "\n",
    "        # combined loss (vision_KD + text_KD + CE)\n",
    "        loss = (alpha * ce_loss + (1 - alpha) * vision_kd_loss) + gamma * text_kd_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(student_logits_base, labels)\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        logit_scale = student_model.logit_scale.exp()\n",
    "        img_feats, text_feats = model(images)\n",
    "        logits = logit_scale * img_feats @ text_feats.t()\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(logits, labels)\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    base_indices,\n",
    "    epochs=10,\n",
    "    patience=3,\n",
    "    alpha=0.5,\n",
    "    gamma=0.2,\n",
    "    temperature=2,\n",
    "    model_state_path='adapter_weights.pt'\n",
    "):\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch: {epoch}/{epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            teacher_model, student_model, train_loader, optimizer,\n",
    "            base_indices, alpha, gamma, temperature\n",
    "        )\n",
    "        val_loss, val_acc = eval(student_model, val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"[LR] Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc.: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc.: {val_acc:.4f}\")\n",
    "\n",
    "        # early stopping\n",
    "        if patience < epochs:\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_vision_adapter_state = copy.deepcopy(student_model.vision_adapter.state_dict())\n",
    "                best_ctx_state = copy.deepcopy(student_model.text_adapter.state_dict())\n",
    "                torch.save(best_vision_adapter_state, \"vision_adapter_state.pt\")\n",
    "                torch.save(best_ctx_state, \"ctx_state.pt\")\n",
    "                print(f\"‚úÖ New best model saved (val_acc = {best_val_acc:.4f})\")\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                print(f\"No improvement for {epochs_no_improve} epoch(s)\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"‚èπÔ∏è Early stopping at epoch {epoch} (best val_acc = {best_val_acc:.4f})\")\n",
    "                break\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "# -- Reproducibility utils --\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Settings used for reproducibility purposes.\n",
    "    https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    \"\"\"\n",
    "    # Python & NumPy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Torch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"[INFO] Seed set to {seed}\")\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40062b4a",
   "metadata": {
    "id": "40062b4a"
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c01aa",
   "metadata": {
    "id": "067c01aa"
   },
   "source": [
    "### CLIP-Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81e098",
   "metadata": {
    "id": "4f81e098"
   },
   "outputs": [],
   "source": [
    "class VisionAdapter(nn.Module):\n",
    "    def __init__(self, c_in, reduction=4, dropout=0.5):\n",
    "        super(VisionAdapter, self).__init__()\n",
    "\n",
    "        hidden_dim = c_in // reduction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=c_in, out_features=hidden_dim, bias=False),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=c_in, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.net(x)  # residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextAdapter(nn.Module):\n",
    "    def __init__(self, classnames, clip_model, n_ctx=16):\n",
    "        super(TextAdapter, self).__init__()\n",
    "        self.classnames = classnames\n",
    "        self.clip_model = clip_model\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]  # context dimension\n",
    "        self.ctx = nn.Parameter(torch.empty(n_ctx, ctx_dim))  # tokens to be learned\n",
    "        nn.init.normal_(self.ctx, std=0.02)\n",
    "\n",
    "        prompt_prefix = \" \".join(['X'] * n_ctx)\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        temp = 'a photo of a {}, a type of flower.'\n",
    "        prompts = [temp.format(c) for c in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + p for p in prompts]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.clip_model.token_embedding(tokenized_prompts).type(self.dtype)\n",
    "\n",
    "        # structure is: [BOS] [class tokens ... ] [EOS]\n",
    "        self.register_buffer(\"token_prefix\", embeddings[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embeddings[:, 1 + n_ctx :, :])  # class tokens + EOS\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        # append the learnable context to the end of the token embeddings\n",
    "        ctx = self.ctx.unsqueeze(0).expand(token_embeddings.shape[0], -1, -1)\n",
    "        return torch.cat([\n",
    "            self.token_prefix,  # BOS\n",
    "            ctx,  # learned context\n",
    "            self.token_suffix,  # class tokens + EOS\n",
    "        ], dim=1).type(self.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a0d3c",
   "metadata": {
    "id": "260a0d3c"
   },
   "source": [
    "### Custom CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5eb64",
   "metadata": {
    "id": "69b5eb64"
   },
   "outputs": [],
   "source": [
    "class CustomCLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        classnames: list[str],\n",
    "        clip_model: nn.Module,\n",
    "        vision_adapter_args,\n",
    "        text_adapter_args,\n",
    "        vision_ratio: float = 0.2,\n",
    "        vision_adapter_state=None,\n",
    "        text_adapter_state=None \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.classnames = classnames\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.clip_model = clip_model\n",
    "        self.vision_ratio = vision_ratio\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.image_encoder = clip_model.visual\n",
    "\n",
    "        # create and attach adapters\n",
    "        vision_feature_dim = self.image_encoder.output_dim\n",
    "        self.vision_adapter = VisionAdapter(vision_feature_dim, **vision_adapter_args)\n",
    "        self.text_adapter = TextAdapter(classnames, clip_model, **text_adapter_args)\n",
    "\n",
    "        if vision_adapter_state:\n",
    "            self.vision_adapter.load_state_dict(vision_adapter_state)\n",
    "\n",
    "        if text_adapter_state:\n",
    "            self.text_adapter.load_state_dict(text_adapter_state)\n",
    "\n",
    "    def encode_image_with_adapter(self, image):\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        adapted_features = self.vision_adapter(image_features.float()).type(self.dtype)\n",
    "        combined_features = self.vision_ratio * adapted_features + (1 - self.vision_ratio) * image_features\n",
    "        # norm & return\n",
    "        return combined_features / combined_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def encode_text_with_adapter(self):\n",
    "        text_embeddings = self.clip_model.token_embedding(\n",
    "            self.text_adapter.tokenized_prompts).type(self.dtype)\n",
    "        adapted_embeddings = self.text_adapter(text_embeddings)\n",
    "\n",
    "        # pass through CLIP text transformer\n",
    "        x = adapted_embeddings + self.clip_model.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x).type(self.dtype)\n",
    "\n",
    "        text_features = x[torch.arange(x.shape[0]), self.text_adapter.tokenized_prompts.argmax(dim=-1)] @ self.clip_model.text_projection\n",
    "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        # compute (normalized) text & image feats.\n",
    "        image_features = self.encode_image_with_adapter(images.type(self.dtype))\n",
    "        text_features = self.encode_text_with_adapter()\n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298169e",
   "metadata": {
    "id": "1298169e"
   },
   "source": [
    "# Preparing the Dataset and CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769719d",
   "metadata": {
    "id": "1769719d"
   },
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Inspect classes\n",
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold',\n",
    "    'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    "    'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris',\n",
    "    'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily',\n",
    "    'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth',\n",
    "    'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william',\n",
    "    'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    "    'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose',\n",
    "    'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue',\n",
    "    'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion',\n",
    "    'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium',\n",
    "    'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia',\n",
    "    'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy',\n",
    "    'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    "    'gazania', 'azalea', 'water lily', 'rose', 'thorn apple',\n",
    "    'morning glory', 'passion flower', 'lotus lotus', 'toad lily', 'anthurium',\n",
    "    'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose',\n",
    "    'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily',\n",
    "    'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    "    'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower',\n",
    "    'trumpet creeper', 'blackberry lily'\n",
    "]\n",
    "\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=DEVICE)\n",
    "\n",
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "# Create data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_base,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_base,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080209",
   "metadata": {
    "id": "cd080209"
   },
   "source": [
    "# Preparing the custom Model + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52173f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2c52173f"
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "patience = 5\n",
    "lr = 2e-3\n",
    "wd = 1e-4\n",
    "# vision adapter hyperparams.\n",
    "vision_ratio = 0.2  # original paper is 0.2\n",
    "vision_reduction = 16\n",
    "vision_dropout = 0.4\n",
    "vision_adapter_args = {\"reduction\": vision_reduction, \"dropout\": vision_dropout}\n",
    "# CoOp hyperparam\n",
    "n_ctx = 16\n",
    "text_adapter_args = {\"n_ctx\": n_ctx}\n",
    "\n",
    "# loss weights hyperparams.\n",
    "# alpha * CE + (1-alpha) * visual_KD + gamma * text_KD\n",
    "alpha = 0.3  \n",
    "gamma = 0.4\n",
    "temperature = 3\n",
    "text_adapter_args = {}\n",
    "\n",
    "# defining teacher / student models\n",
    "teacher_model = clip_model\n",
    "teacher_model.eval()\n",
    "\n",
    "# freezing teacher (CLIP base)\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "student_model = CustomCLIP(\n",
    "    CLASS_NAMES,\n",
    "    clip_model,\n",
    "    vision_adapter_args=vision_adapter_args,\n",
    "    text_adapter_args=text_adapter_args,\n",
    "    vision_ratio=vision_ratio\n",
    ").to(DEVICE)\n",
    "\n",
    "trainable_params = chain(\n",
    "    student_model.vision_adapter.parameters(),\n",
    "    student_model.text_adapter.parameters(),\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=wd)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "history = train_loop(\n",
    "    teacher_model, student_model, train_loader, val_loader,\n",
    "    optimizer, scheduler, epochs=epochs, patience=patience,\n",
    "    base_indices=base_classes, alpha=alpha, gamma=gamma,\n",
    "    temperature=temperature\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4dc1a",
   "metadata": {
    "id": "83b4dc1a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Used as reference for plotting on the x-axis\n",
    "x = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, history['train_loss'], label='Train Loss')\n",
    "plt.plot(x, history['val_loss'], label='Val Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Training vs Evaluation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(x, history['val_acc'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Training vs Evaluation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac306b-fab1-4ec3-885d-7fe5554559e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "# best_model_state = copy.deepcopy(student_model.adapter.state_dict())\n",
    "# torch.save(best_model_state, run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce59b30",
   "metadata": {
    "id": "dce59b30"
   },
   "source": [
    "# Evaluation on best model found during training (based on validation accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc571aba",
   "metadata": {
    "id": "bc571aba"
   },
   "outputs": [],
   "source": [
    "# loading best saved model weights\n",
    "vision_adapter_state = torch.load(\"vision_adapter_state.pt\", weights_only=True)\n",
    "text_adapter_state = torch.load(\"ctx_state.pt\", weights_only=True)\n",
    "best_student_model = CustomCLIP(\n",
    "    CLASS_NAMES,\n",
    "    clip_model,\n",
    "    vision_adapter_args,\n",
    "    text_adapter_args,\n",
    "    vision_ratio,\n",
    "    vision_adapter_state=vision_adapter_state,  # learned CLIP-adapter on the vision encoder\n",
    "    text_adapter_state=text_adapter_state  # learned ctx\n",
    ").to(DEVICE)\n",
    "\n",
    "# ---- Compute accuracy on base and novel classes ----\n",
    "baseLoader = torch.utils.data.DataLoader(\n",
    "    test_base,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g\n",
    ")\n",
    "novelLoader = torch.utils.data.DataLoader(\n",
    "    test_novel,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "print()\n",
    "_, base_accuracy = eval(best_student_model, baseLoader)\n",
    "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
    "_, novel_accuracy = eval(best_student_model, novelLoader)\n",
    "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
    "print(f\"üîç Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
