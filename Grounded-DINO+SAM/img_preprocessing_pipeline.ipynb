{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce73f49",
      "metadata": {
        "id": "0ce73f49"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade -q git+https://github.com/huggingface/transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "380a85f5",
      "metadata": {
        "id": "380a85f5"
      },
      "source": [
        "# Adding Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68dd87f1",
      "metadata": {
        "id": "68dd87f1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, List, Dict, Optional, Union, Tuple\n",
        "import torchvision\n",
        "from torchvision.datasets.flowers102 import Flowers102\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from transformers import AutoModelForMaskGeneration, AutoProcessor, pipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18672815",
      "metadata": {
        "id": "18672815"
      },
      "source": [
        "## Result Utils\n",
        "We'll store the detection results of Grounding DINO in a dedicated python dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "037f59a8",
      "metadata": {
        "id": "037f59a8"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class BoundingBox:\n",
        "    xmin: int\n",
        "    ymin: int\n",
        "    xmax: int\n",
        "    ymax: int\n",
        "\n",
        "    @property\n",
        "    def xyxy(self) -> List[float]:\n",
        "        return [self.xmin, self.ymin, self.xmax, self.ymax]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DetectionResult:\n",
        "    score: float\n",
        "    label: str\n",
        "    box: BoundingBox\n",
        "    mask: Optional[np.array] = None\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, detection_dict: Dict) -> 'DetectionResult':\n",
        "        return cls(score=detection_dict['score'],\n",
        "                   label=detection_dict['label'],\n",
        "                   box=BoundingBox(xmin=detection_dict['box']['xmin'],\n",
        "                                   ymin=detection_dict['box']['ymin'],\n",
        "                                   xmax=detection_dict['box']['xmax'],\n",
        "                                   ymax=detection_dict['box']['ymax']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0747f7c9",
      "metadata": {
        "id": "0747f7c9"
      },
      "source": [
        "## Plot Utils\n",
        "Used to draw the detection results of Grounding DINO onto the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a924cd3d",
      "metadata": {
        "id": "a924cd3d"
      },
      "outputs": [],
      "source": [
        "def annotate(image: Union[Image.Image, np.ndarray], detection_results: List[DetectionResult]) -> np.ndarray:\n",
        "    # Convert PIL Image to OpenCV format\n",
        "    image_cv2 = np.array(image) if isinstance(image, Image.Image) else image\n",
        "    image_cv2 = cv2.cvtColor(image_cv2, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Iterate over detections and add bounding boxes and masks\n",
        "    for detection in detection_results:\n",
        "        label = detection.label\n",
        "        score = detection.score\n",
        "        box = detection.box\n",
        "        mask = detection.mask\n",
        "\n",
        "        # Sample a random color for each detection\n",
        "        color = np.random.randint(0, 256, size=3)\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(image_cv2, (box.xmin, box.ymin),\n",
        "                      (box.xmax, box.ymax), color.tolist(), 2)\n",
        "        cv2.putText(image_cv2, f'{label}: {score:.2f}', (box.xmin,\n",
        "                    box.ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color.tolist(), 2)\n",
        "\n",
        "        # If mask is available, apply it\n",
        "        if mask is not None:\n",
        "            # Convert mask to uint8\n",
        "            mask_uint8 = (mask * 255).astype(np.uint8)\n",
        "            contours, _ = cv2.findContours(\n",
        "                mask_uint8, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "            cv2.drawContours(image_cv2, contours, -1, color.tolist(), 2)\n",
        "\n",
        "    return cv2.cvtColor(image_cv2, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "\n",
        "def plot_detections(\n",
        "    image: Union[Image.Image, np.ndarray],\n",
        "    detections: List[DetectionResult],\n",
        "    save_name: Optional[str] = None\n",
        ") -> None:\n",
        "    annotated_image = annotate(image, detections)\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.axis('off')\n",
        "    if save_name:\n",
        "        plt.savefig(save_name, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def random_named_css_colors(num_colors: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    Returns a list of randomly selected named CSS colors.\n",
        "\n",
        "    Args:\n",
        "    - num_colors (int): Number of random colors to generate.\n",
        "\n",
        "    Returns:\n",
        "    - list: List of randomly selected named CSS colors.\n",
        "    \"\"\"\n",
        "    # List of named CSS colors\n",
        "    named_css_colors = [\n",
        "        'aliceblue', 'antiquewhite', 'aqua', 'aquamarine', 'azure', 'beige', 'bisque', 'black', 'blanchedalmond',\n",
        "        'blue', 'blueviolet', 'brown', 'burlywood', 'cadetblue', 'chartreuse', 'chocolate', 'coral', 'cornflowerblue',\n",
        "        'cornsilk', 'crimson', 'cyan', 'darkblue', 'darkcyan', 'darkgoldenrod', 'darkgray', 'darkgreen', 'darkgrey',\n",
        "        'darkkhaki', 'darkmagenta', 'darkolivegreen', 'darkorange', 'darkorchid', 'darkred', 'darksalmon', 'darkseagreen',\n",
        "        'darkslateblue', 'darkslategray', 'darkslategrey', 'darkturquoise', 'darkviolet', 'deeppink', 'deepskyblue',\n",
        "        'dimgray', 'dimgrey', 'dodgerblue', 'firebrick', 'floralwhite', 'forestgreen', 'fuchsia', 'gainsboro', 'ghostwhite',\n",
        "        'gold', 'goldenrod', 'gray', 'green', 'greenyellow', 'grey', 'honeydew', 'hotpink', 'indianred', 'indigo', 'ivory',\n",
        "        'khaki', 'lavender', 'lavenderblush', 'lawngreen', 'lemonchiffon', 'lightblue', 'lightcoral', 'lightcyan', 'lightgoldenrodyellow',\n",
        "        'lightgray', 'lightgreen', 'lightgrey', 'lightpink', 'lightsalmon', 'lightseagreen', 'lightskyblue', 'lightslategray',\n",
        "        'lightslategrey', 'lightsteelblue', 'lightyellow', 'lime', 'limegreen', 'linen', 'magenta', 'maroon', 'mediumaquamarine',\n",
        "        'mediumblue', 'mediumorchid', 'mediumpurple', 'mediumseagreen', 'mediumslateblue', 'mediumspringgreen', 'mediumturquoise',\n",
        "        'mediumvioletred', 'midnightblue', 'mintcream', 'mistyrose', 'moccasin', 'navajowhite', 'navy', 'oldlace', 'olive',\n",
        "        'olivedrab', 'orange', 'orangered', 'orchid', 'palegoldenrod', 'palegreen', 'paleturquoise', 'palevioletred', 'papayawhip',\n",
        "        'peachpuff', 'peru', 'pink', 'plum', 'powderblue', 'purple', 'rebeccapurple', 'red', 'rosybrown', 'royalblue', 'saddlebrown',\n",
        "        'salmon', 'sandybrown', 'seagreen', 'seashell', 'sienna', 'silver', 'skyblue', 'slateblue', 'slategray', 'slategrey',\n",
        "        'snow', 'springgreen', 'steelblue', 'tan', 'teal', 'thistle', 'tomato', 'turquoise', 'violet', 'wheat', 'white',\n",
        "        'whitesmoke', 'yellow', 'yellowgreen'\n",
        "    ]\n",
        "\n",
        "    # Sample random named CSS colors\n",
        "    return random.sample(named_css_colors, min(num_colors, len(named_css_colors)))\n",
        "\n",
        "\n",
        "def plot_detections_plotly(\n",
        "    image: np.ndarray,\n",
        "    detections: List[DetectionResult],\n",
        "    class_colors: Optional[Dict[str, str]] = None\n",
        ") -> None:\n",
        "    # If class_colors is not provided, generate random colors for each class\n",
        "    if class_colors is None:\n",
        "        num_detections = len(detections)\n",
        "        colors = random_named_css_colors(num_detections)\n",
        "        class_colors = {}\n",
        "        for i in range(num_detections):\n",
        "            class_colors[i] = colors[i]\n",
        "\n",
        "    fig = px.imshow(image)\n",
        "\n",
        "    # Add bounding boxes\n",
        "    shapes = []\n",
        "    annotations = []\n",
        "    for idx, detection in enumerate(detections):\n",
        "        label = detection.label\n",
        "        box = detection.box\n",
        "        score = detection.score\n",
        "        mask = detection.mask\n",
        "\n",
        "        polygon = mask_to_polygon(mask)\n",
        "\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[point[0] for point in polygon] + [polygon[0][0]],\n",
        "            y=[point[1] for point in polygon] + [polygon[0][1]],\n",
        "            mode='lines',\n",
        "            line=dict(color=class_colors[idx], width=2),\n",
        "            fill='toself',\n",
        "            name=f\"{label}: {score:.2f}\"\n",
        "        ))\n",
        "\n",
        "        xmin, ymin, xmax, ymax = box.xyxy\n",
        "        shape = [\n",
        "            dict(\n",
        "                type=\"rect\",\n",
        "                xref=\"x\", yref=\"y\",\n",
        "                x0=xmin, y0=ymin,\n",
        "                x1=xmax, y1=ymax,\n",
        "                line=dict(color=class_colors[idx])\n",
        "            )\n",
        "        ]\n",
        "        annotation = [\n",
        "            dict(\n",
        "                x=(xmin+xmax) // 2, y=(ymin+ymax) // 2,\n",
        "                xref=\"x\", yref=\"y\",\n",
        "                text=f\"{label}: {score:.2f}\",\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        shapes.append(shape)\n",
        "        annotations.append(annotation)\n",
        "\n",
        "    # Update layout\n",
        "    button_shapes = [\n",
        "        dict(label=\"None\", method=\"relayout\", args=[\"shapes\", []])]\n",
        "    button_shapes = button_shapes + [\n",
        "        dict(label=f\"Detection {idx+1}\", method=\"relayout\", args=[\"shapes\", shape]) for idx, shape in enumerate(shapes)\n",
        "    ]\n",
        "    button_shapes = button_shapes + \\\n",
        "        [dict(label=\"All\", method=\"relayout\", args=[\"shapes\", sum(shapes, [])])]\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis=dict(visible=False),\n",
        "        yaxis=dict(visible=False),\n",
        "        # margin=dict(l=0, r=0, t=0, b=0),\n",
        "        showlegend=True,\n",
        "        updatemenus=[\n",
        "            dict(\n",
        "                type=\"buttons\",\n",
        "                direction=\"up\",\n",
        "                buttons=button_shapes\n",
        "            )\n",
        "        ],\n",
        "        legend=dict(\n",
        "            orientation=\"h\",\n",
        "            yanchor=\"bottom\",\n",
        "            y=1.02,\n",
        "            xanchor=\"right\",\n",
        "            x=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Show plot\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38623f6b",
      "metadata": {
        "id": "38623f6b"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b40ff9",
      "metadata": {
        "id": "f7b40ff9"
      },
      "outputs": [],
      "source": [
        "def mask_to_polygon(mask: np.ndarray) -> List[List[int]]:\n",
        "    # Find contours in the binary mask\n",
        "    contours, _ = cv2.findContours(mask.astype(\n",
        "        np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Find the contour with the largest area\n",
        "    largest_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    # Extract the vertices of the contour\n",
        "    polygon = largest_contour.reshape(-1, 2).tolist()\n",
        "\n",
        "    return polygon\n",
        "\n",
        "\n",
        "def polygon_to_mask(polygon: List[Tuple[int, int]], image_shape: Tuple[int, int]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Convert a polygon to a segmentation mask.\n",
        "\n",
        "    Args:\n",
        "    - polygon (list): List of (x, y) coordinates representing the vertices of the polygon.\n",
        "    - image_shape (tuple): Shape of the image (height, width) for the mask.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Segmentation mask with the polygon filled.\n",
        "    \"\"\"\n",
        "    # Create an empty mask\n",
        "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
        "\n",
        "    # Convert polygon to an array of points\n",
        "    pts = np.array(polygon, dtype=np.int32)\n",
        "\n",
        "    # Fill the polygon with white color (255)\n",
        "    cv2.fillPoly(mask, [pts], color=(255,))\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def load_image(image_str: str) -> Image.Image:\n",
        "    if image_str.startswith(\"http\"):\n",
        "        image = Image.open(requests.get(\n",
        "            image_str, stream=True).raw).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_str).convert(\"RGB\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def get_boxes(results: DetectionResult) -> List[List[List[float]]]:\n",
        "    boxes = []\n",
        "    for result in results:\n",
        "        xyxy = result.box.xyxy\n",
        "        boxes.append(xyxy)\n",
        "\n",
        "    return [boxes]\n",
        "\n",
        "\n",
        "def refine_masks(masks: torch.BoolTensor, polygon_refinement: bool = False) -> List[np.ndarray]:\n",
        "    masks = masks.cpu().float()\n",
        "    masks = masks.permute(0, 2, 3, 1)\n",
        "    masks = masks.mean(axis=-1)\n",
        "    masks = (masks > 0).int()\n",
        "    masks = masks.numpy().astype(np.uint8)\n",
        "    masks = list(masks)\n",
        "\n",
        "    if polygon_refinement:\n",
        "        for idx, mask in enumerate(masks):\n",
        "            shape = mask.shape\n",
        "            polygon = mask_to_polygon(mask)\n",
        "            mask = polygon_to_mask(polygon, shape)\n",
        "            masks[idx] = mask\n",
        "\n",
        "    return masks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee7e48b6",
      "metadata": {
        "id": "ee7e48b6"
      },
      "source": [
        "# Grounded SAM (Segment Anything)\n",
        "\n",
        "The approach involves two steps:\n",
        "1. Use Grounding DINO to detect a given set of texts in the image. The output is a set of bounding boxes.\n",
        "2. Prompt Segment Anything (SAM) with the bounding boxes, for which the model will output segmentation masks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33ee4fa",
      "metadata": {
        "id": "d33ee4fa"
      },
      "outputs": [],
      "source": [
        "def detect(\n",
        "    image: Image.Image,\n",
        "    labels: List[str],\n",
        "    threshold: float = 0.3,\n",
        "    detector_id: Optional[str] = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Use Grounding DINO to detect a set of labels in an image in a zero-shot fashion.\n",
        "    \"\"\"\n",
        "    detector_id = detector_id if detector_id is not None else \"IDEA-Research/grounding-dino-tiny\"\n",
        "    object_detector = pipeline(\n",
        "        model=detector_id, task=\"zero-shot-object-detection\", device=device)\n",
        "\n",
        "    labels = [label if label.endswith(\".\") else label+\".\" for label in labels]\n",
        "\n",
        "    results = object_detector(\n",
        "        image,  candidate_labels=labels, threshold=threshold)\n",
        "    results = [DetectionResult.from_dict(result) for result in results]\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def segment(\n",
        "    image: Image.Image,\n",
        "    detection_results: List[Dict[str, Any]],\n",
        "    polygon_refinement: bool = False,\n",
        "    segmenter_id: Optional[str] = None\n",
        ") -> List[DetectionResult]:\n",
        "    \"\"\"\n",
        "    Use Segment Anything (SAM) to generate masks given an image + a set of bounding boxes.\n",
        "    \"\"\"\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    segmenter_id = segmenter_id if segmenter_id is not None else \"facebook/sam-vit-base\"\n",
        "\n",
        "    segmentator = AutoModelForMaskGeneration.from_pretrained(\n",
        "        segmenter_id).to(device)\n",
        "    processor = AutoProcessor.from_pretrained(segmenter_id)\n",
        "\n",
        "    boxes = get_boxes(detection_results)\n",
        "    inputs = processor(images=image, input_boxes=boxes,\n",
        "                       return_tensors=\"pt\").to(device)\n",
        "\n",
        "    outputs = segmentator(**inputs)\n",
        "    masks = processor.post_process_masks(\n",
        "        masks=outputs.pred_masks,\n",
        "        original_sizes=inputs.original_sizes,\n",
        "        reshaped_input_sizes=inputs.reshaped_input_sizes\n",
        "    )[0]\n",
        "\n",
        "    masks = refine_masks(masks, polygon_refinement)\n",
        "\n",
        "    for detection_result, mask in zip(detection_results, masks):\n",
        "        detection_result.mask = mask\n",
        "\n",
        "    return detection_results\n",
        "\n",
        "\n",
        "def grounded_segmentation(\n",
        "    image: Image.Image,\n",
        "    labels: List[str],\n",
        "    threshold: float = 0.3,\n",
        "    polygon_refinement: bool = False,\n",
        "    detector_id: Optional[str] = None,\n",
        "    segmenter_id: Optional[str] = None\n",
        ") -> Tuple[np.ndarray, List[DetectionResult]]:\n",
        "    if isinstance(image, str):\n",
        "        image = load_image(image)\n",
        "\n",
        "    detections = detect(image, labels, threshold, detector_id)\n",
        "    detections = segment(image, detections, polygon_refinement, segmenter_id)\n",
        "\n",
        "    return np.array(image), detections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a3fefb",
      "metadata": {
        "id": "43a3fefb"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d88a4c71",
      "metadata": {
        "id": "d88a4c71"
      },
      "outputs": [],
      "source": [
        "image_url = \"./cyclamen_1.jpg\"\n",
        "labels = [\"a flower.\"]\n",
        "threshold = 0.3\n",
        "\n",
        "detector_id = \"IDEA-Research/grounding-dino-tiny\"\n",
        "segmenter_id = \"facebook/sam-vit-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bcd20a5",
      "metadata": {
        "id": "5bcd20a5"
      },
      "outputs": [],
      "source": [
        "image_array, detections = grounded_segmentation(\n",
        "    image=image_url,\n",
        "    labels=labels,\n",
        "    threshold=threshold,\n",
        "    polygon_refinement=True,\n",
        "    detector_id=detector_id,\n",
        "    segmenter_id=segmenter_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "196515f2",
      "metadata": {
        "id": "196515f2"
      },
      "outputs": [],
      "source": [
        "plot_detections(image_array, detections, \"example_segmentation.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618824a4",
      "metadata": {
        "id": "618824a4"
      },
      "outputs": [],
      "source": [
        "plot_detections_plotly(image_array, detections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b920b51",
      "metadata": {
        "id": "9b920b51"
      },
      "outputs": [],
      "source": [
        "def save_segmented_image(\n",
        "    image: Union[np.ndarray, Image.Image],\n",
        "    detection_result: 'DetectionResult',\n",
        "    output_path: str,\n",
        "    background_color: tuple = (0, 0, 0)  # Black background by default\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Save an image with only the segmented region visible on a black background.\n",
        "\n",
        "    Args:\n",
        "        image: Input image as numpy array or PIL Image\n",
        "        detection_result: DetectionResult object containing the mask\n",
        "        output_path: Path where to save the segmented image\n",
        "        background_color: RGB tuple for background color (default: black)\n",
        "    \"\"\"\n",
        "\n",
        "    np.save(output_path.replace(\".jpg\", \"_detection.npy\"), detection_result.mask)\n",
        "    # Convert PIL Image to numpy array if needed\n",
        "    if isinstance(image, Image.Image):\n",
        "        image_array = np.array(image)\n",
        "    else:\n",
        "        image_array = image.copy()\n",
        "\n",
        "    # Ensure image is in RGB format\n",
        "    if len(image_array.shape) == 3 and image_array.shape[2] == 3:\n",
        "        # Already RGB\n",
        "        pass\n",
        "    elif len(image_array.shape) == 3 and image_array.shape[2] == 4:\n",
        "        # Convert RGBA to RGB\n",
        "        image_array = image_array[:, :, :3]\n",
        "    else:\n",
        "        raise ValueError(\"Image must be RGB or RGBA format\")\n",
        "\n",
        "    # Check if mask exists\n",
        "    if detection_result.mask is None:\n",
        "        raise ValueError(\n",
        "            \"DetectionResult object must have a mask to create segmented image\")\n",
        "\n",
        "    # Get the mask\n",
        "    mask = detection_result.mask\n",
        "\n",
        "    # Handle different mask formats\n",
        "    if isinstance(mask, np.ndarray):\n",
        "        # If mask is 3D (batch, height, width), take the first one\n",
        "        if len(mask.shape) == 3:\n",
        "            mask = mask[0]\n",
        "        # Convert to boolean if it's not already\n",
        "        if mask.dtype != bool:\n",
        "            mask = mask > 0.5  # Threshold for binary mask\n",
        "    else:\n",
        "        raise ValueError(\"Mask must be a numpy array\")\n",
        "\n",
        "    # Ensure mask dimensions match image dimensions\n",
        "    if mask.shape[:2] != image_array.shape[:2]:\n",
        "        # Resize mask to match image dimensions\n",
        "        mask = cv2.resize(mask.astype(np.uint8),\n",
        "                          (image_array.shape[1], image_array.shape[0]),\n",
        "                          interpolation=cv2.INTER_NEAREST).astype(bool)\n",
        "\n",
        "    # Create output image with background color\n",
        "    segmented_image = np.full_like(image_array, background_color)\n",
        "\n",
        "    # Apply mask - keep original pixels where mask is True\n",
        "    segmented_image[mask] = image_array[mask]\n",
        "\n",
        "    # Save the image\n",
        "    output_image = Image.fromarray(segmented_image.astype(np.uint8))\n",
        "    output_image.save(output_path)\n",
        "\n",
        "    print(f\"Segmented image saved to: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the segmented foreground image for each of the 10 shots for each class in the Flowers102 dataset"
      ],
      "metadata": {
        "id": "sMpY24VGMpo_"
      },
      "id": "sMpY24VGMpo_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d44b26",
      "metadata": {
        "id": "02d44b26"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir='./data', transform=None) -> tuple[Flowers102,\n",
        "                                                         Flowers102,\n",
        "                                                         Flowers102]:\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = torchvision.datasets.Flowers102(\n",
        "        root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(\n",
        "        root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(\n",
        "        root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def split_data(dataset, base_classes):\n",
        "    # these two lists will store the sample indexes\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "\n",
        "    # we create a set of base classes to compute the test below in O(1)\n",
        "    # this is optional and can be removed\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # here we iterate over sample labels and also get the correspondent sample index\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # here we create the dataset subsets\n",
        "    # the torch Subset is just a wrapper around the dataset\n",
        "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
        "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
        "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset\n",
        "\n",
        "\n",
        "# Get images from Flowers102 dataset, then construct the training set dataloader\n",
        "raw_train, _, _ = get_data(data_dir='./data', transform=None)\n",
        "\n",
        "current_shot = 1\n",
        "# Process each image in the train loader and apply the segmentation pipeline\n",
        "for idx, (img, label) in enumerate(raw_train):\n",
        "  image_array, detections = grounded_segmentation(\n",
        "      img,\n",
        "      labels=labels,\n",
        "      threshold=0.35\n",
        "  )\n",
        "  save_segmented_image(\n",
        "      image_array,\n",
        "      detections[0],\n",
        "      f\"./segmented/l{label}_s{current_shot}.jpg\"\n",
        "  )\n",
        "\n",
        "  current_shot = idx % 10 + 1\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}