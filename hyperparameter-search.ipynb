{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c162991",
   "metadata": {},
   "source": [
    "# Project Motivation\n",
    "\n",
    "## Related Works\n",
    "- **Prompt-Tuning** methods such as [CoOp](https://arxiv.org/pdf/2109.01134) and [CoCoOp](https://arxiv.org/pdf/2203.05557) are prone to overfitting on the few-shot examples available per class. While they address the problem from the prompt level i.e. language modality, they do not consider, hence leverage CLIP's shared embedding space between both vision and language modalities.\n",
    "\n",
    "- On the other hand, **Visual Adapters** like [CLIP-Adapter](https://arxiv.org/pdf/2110.04544) tackle the problem of few-shot-learning from the visual level by learning a residual on vision features (i.e. CLS token embedding from the ViT architecture) through a bottleneck layer. However, much like prompt-tuning methods (CoOp and CoCoOp), visual adapters are also prone to overfitting on the available few-shots per class and alone, lack linguistics context provided by the textual prompts.\n",
    "\n",
    "\n",
    "## Analyses to Motivate the Work\n",
    "To motivate the proposed method, two analyses were performed on CLIP-Adapter and CoOp after reproducing their results:\n",
    "1. The first analysis investigated if performance degradation on unseen classes was tied to distance in embedding space between the *learned* prompt (concatenatic CoOp's context tokens with the class tokens) and the *original* frozen prompt.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"https://i.postimg.cc/28r653Yw/clip-analysis.png\" alt=\"image_caption\" width=\"900\">\n",
    "\n",
    "*Figure 1: Comparison of embedding distance vs. accuracy. Drifting too far from the original prompt correlates with reduced generalization accuracy on unseen classes.*\n",
    "\n",
    "</div>\n",
    "\n",
    "2. Similarly, the second analysis assesses how visual features given by CLIP's frozen encoder diverged in embedding space from the learned representations by CLIP-Adapter i.e., the adapted features. The goal was to determine if high divergence would correlate with a drop in performance on unseen classes. \n",
    "\n",
    "    The results are shown in the figure below:\n",
    "   \n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "<img src=\"https://i.postimg.cc/L4z5RLcg/clip-adapter-analysis.png\" alt=\"image_caption\" width=\"900\">\n",
    "\n",
    "*Figure 2: Empirical analysis showing that divergence in embedding space between adapted and non-adapted visual features is strongly correlated with performance degradation on novel classes*\n",
    "\n",
    "</div>\n",
    "\n",
    "## Problem Statement\n",
    "The issue with existing approaches is that, in base-to-novel settings, they do not perform well as they:\n",
    "1. Fail to fully exploit the shared embedding space between both vision and language modalities.\n",
    "2. Lack active regularization to prevent overfitting on base classes and keep generalization performance on novel classes compared to CLIP zero-shot performance.\n",
    "\n",
    "## High Level Idea\n",
    "The objective of this project is to:\n",
    "1. Combine the two above mentioned approaches to leverage the shared embedding space between both vision and language modalities\n",
    "2. Constrain the learning of context tokens (for the language side) to yield text embeddings too far from frozen CLIP embeddings\n",
    "3. Regularize the learning of adapted visual features to prevent them from diverging too much from frozen CLIP embeddings.\n",
    "\n",
    "All of these while still managing to learn task-specific features that helps discriminate better among (seen) base classes during training.\n",
    "\n",
    "## Method\n",
    "This project implements a teacher-student framework.\n",
    "- The teacher is a frozen CLIP model.\n",
    "- The student builds upon the CLIP model with both additional learnable prompt tokens  and a learnable visual adapter i.e. bottleneck head.\n",
    "\n",
    "Given an input image $\\mathbf{x}$, a visual encoder $\\phi(\\cdot)$, a text encoder $\\psi_(\\cdot)$, a set of hand-crafted prompts $\\mathbf{P}$ and a set of learnable context tokens $\\mathbf{\\text{ctx}}$, the student is trained to simultaneusly:\n",
    "- minimize the distance between the learned, adapted visual features $(\\phi_{\\text{student}}(x) )$ and the visual features produced by frozen CLIP's visual encoder $(\\mathbf{V}_{\\text{teacher}})$ via a Kullback-Leibler divergence between the student's and teacher's logits and \n",
    "- minimize the distance between the learned, adapted text embeddings $(\\psi_{\\text{student}}([\\mathbf{\\text{ctx}}, \\mathbf{P}]))$ with CoOp's context tokens and the text embeddings produced by frozen CLIP's text encoder $(\\psi_{\\text{teacher}}(\\mathbf{P}))$ via a simple cosine similarity between the two.\n",
    "\n",
    "More specifically, the final loss function is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{student}} = \\alpha \\mathcal{L}_{\\text {CE}} + (1-\\alpha) \\mathcal{L}_{\\text{vis}} + \\beta \\mathcal{L}_{\\text{txt}}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{L}_{\\text CE}$ is the standard **cross-entropy loss** and $\\mathcal{L}_{\\text{vis}}$ is the **Visual Distillation Loss** defined as the Kullback-Leibler divergence between the student's (vision) and teacher's logits defined as:\n",
    "$$\n",
    "\\mathcal{L}_{\\text{vis}} = \\text{KL}(\\mathbf{p}_{\\text{student}} || \\mathbf{p}_{\\text{teacher}})\n",
    "$$ \n",
    "where $\\mathbf{p}_{\\text{teacher}} = \\cos(\\phi_{\\text{teacher}}(x), \\psi_{\\text{teacher}}([\\mathbf{\\text{ctx}}, \\mathbf{P}]))$ and $\\mathbf{p}_{\\text{student}} = \\cos(\\phi_{\\text{student}}(x), \\psi_{\\text{student}}([\\mathbf{\\text{ctx}}, \\mathbf{P}]))$.\n",
    "\n",
    "$\\mathcal{L}_{\\text{txt}}$ is the cosine similarity between the student's embeddings produced with CoOp's context tokens and the teacher's embeddings produced without any addition to the hand-crafted prompt. Formally, it is defined as: \n",
    "$$\n",
    "\\mathcal{L}_{\\text{txt}} = 1 - \\cos(\\psi_{\\text{student}}([\\mathbf{\\text{ctx}}, \\mathbf{P}]), \\psi_{\\text{teacher}}(\\mathbf{P}))\n",
    "$$\n",
    "\n",
    "Note: Although the KL loss operates on the final logits, rather than directly on the visual feature vectors, it indirectly enforces the student's visual features to be close to the teacher's by matching the output distribution structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777dd329",
   "metadata": {
    "id": "777dd329"
   },
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X_HTZmlp7OSW",
   "metadata": {
    "id": "X_HTZmlp7OSW"
   },
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76baca5d",
   "metadata": {
    "id": "76baca5d"
   },
   "outputs": [],
   "source": [
    "# @title Imports\n",
    "\n",
    "import os\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets.flowers102 import Flowers102\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "import clip\n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy\n",
    "from itertools import chain\n",
    "import json\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 43"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736b87a1",
   "metadata": {
    "id": "736b87a1"
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb152309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Generic Utilities\n",
    "# -- Reproducibility utils --\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Settings used for reproducibility purposes.\n",
    "    https://pytorch.org/docs/stable/notes/randomness.html\n",
    "    \"\"\"\n",
    "    # Python & NumPy\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # Torch determinism\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    print(f\"[INFO] Seed set to {seed}\")\n",
    "\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def base_novel_categories(dataset) -> tuple[list[int], list[int]]:\n",
    "    # set returns the unique set of all dataset classes\n",
    "    all_classes = set(dataset._labels)\n",
    "    # and let's count them\n",
    "    num_classes = len(all_classes)\n",
    "\n",
    "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
    "    # then we slice the list in half and generate base and novel category lists\n",
    "    base_classes = list(range(num_classes))[:num_classes // 2]\n",
    "    novel_classes = list(range(num_classes))[num_classes // 2:]\n",
    "    return base_classes, novel_classes\n",
    "\n",
    "\n",
    "def harmonic_mean(base_accuracy, novel_accuracy):\n",
    "    numerator = 2\n",
    "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
    "    hm = numerator / denominator\n",
    "    return hm\n",
    "\n",
    "\n",
    "def get_data(data_dir='./data', transform=None) -> tuple[Flowers102,\n",
    "                                                         Flowers102,\n",
    "                                                         Flowers102]:\n",
    "    \"\"\"Load Flowers102 train, validation and test sets.\n",
    "    Args:\n",
    "        data_dir (str): Directory where the dataset will be stored.\n",
    "        transform (torch.Compose)\n",
    "    Returns:\n",
    "        tuple: A tuple containing the train, validation, and test sets.\n",
    "    \"\"\"\n",
    "    train = Flowers102(\n",
    "        root=data_dir, split=\"train\", download=True, transform=transform)\n",
    "    val = Flowers102(\n",
    "        root=data_dir, split=\"val\", download=True, transform=transform)\n",
    "    test = Flowers102(\n",
    "        root=data_dir, split=\"test\", download=True, transform=transform)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "def split_data(dataset, base_classes):\n",
    "    # these two lists will store the sample indexes\n",
    "    base_categories_samples = []\n",
    "    novel_categories_samples = []\n",
    "\n",
    "    # we create a set of base classes to compute the test below in O(1)\n",
    "    # this is optional and can be removed\n",
    "    base_set = set(base_classes)\n",
    "\n",
    "    # here we iterate over sample labels and also get the correspondent sample index\n",
    "    for sample_id, label in enumerate(dataset._labels):\n",
    "        if label in base_set:\n",
    "            base_categories_samples.append(sample_id)\n",
    "        else:\n",
    "            novel_categories_samples.append(sample_id)\n",
    "\n",
    "    # here we create the dataset subsets\n",
    "    # the torch Subset is just a wrapper around the dataset\n",
    "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
    "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
    "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
    "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
    "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
    "    return base_dataset, novel_dataset\n",
    "\n",
    "\n",
    "def accuracy(logits: torch.Tensor, labels: torch.Tensor) -> float:\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    return (preds == labels).float().mean().item()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        logit_scale = student_model.logit_scale.exp()\n",
    "        img_feats, text_feats = model(images)\n",
    "        logits = 100.0 * img_feats @ text_feats.t()\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(logits, labels)\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8de9a",
   "metadata": {
    "id": "53e8de9a"
   },
   "outputs": [],
   "source": [
    "# @title Training Utilities\n",
    "def train_one_epoch(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    dataloader,\n",
    "    optimizer,\n",
    "    base_indices,\n",
    "    teacher_text_feats,\n",
    "    alpha=0.5,\n",
    "    beta=0.2,\n",
    "    temperature=2.0\n",
    "):\n",
    "    teacher_model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    for imgs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        # student forward pass\n",
    "        student_img_feats, student_text_feats = student_model(imgs)  # both with shape: [B, 102]\n",
    "        # print(\"student text_features shape: \", student_text_feats.shape)\n",
    "\n",
    "        # teacher forward pass\n",
    "        with torch.no_grad():\n",
    "            dtype = teacher_model.dtype\n",
    "            teacher_img_feats = teacher_model.visual(imgs.type(dtype))\n",
    "            teacher_img_feats /= teacher_img_feats.norm(dim=-1, keepdim=True).clamp(min=1e-12)\n",
    "            # print(\"teacher text_features shape: \", teacher_text_feats.shape)\n",
    "\n",
    "        # calculate student and teacher logits\n",
    "        logit_scale = student_model.logit_scale.exp()\n",
    "        student_logits = logit_scale * student_img_feats @ student_text_feats.t()\n",
    "        # note: vision logits are the cos. sim between teacher img feats (non adapted) and student text feats (adapted with CoOp).\n",
    "        teacher_vision_logits = logit_scale * teacher_img_feats @ student_text_feats.t()\n",
    "\n",
    "        student_logits_base = student_logits[:, base_indices]  # logtis for base classes only\n",
    "\n",
    "        # CE loss on base classes only\n",
    "        ce_loss = F.cross_entropy(student_logits_base, labels)\n",
    "\n",
    "        # visual distillation loss (KL div.) with temperature\n",
    "        vision_kl_loss = F.kl_div(\n",
    "            F.log_softmax(student_logits / temperature, dim=-1),\n",
    "            F.softmax(teacher_vision_logits / temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        ) * (temperature ** 2)\n",
    "\n",
    "        # text feature-based KD loss\n",
    "        text_kd_loss = 1 - F.cosine_similarity(\n",
    "            student_text_feats, teacher_text_feats, dim=-1\n",
    "        ).mean()\n",
    "\n",
    "        # combined loss (vision_KD + text_KD + CE)\n",
    "        loss = (alpha * ce_loss + (1 - alpha) * vision_kl_loss) + beta * text_kd_loss # (visual KL + CE + text KD)\n",
    "        #loss = (alpha * ce_loss + (1 - alpha) * vision_kl_loss) # (visual KL + CE)\n",
    "        #loss = ce_loss + beta * text_kd_loss # (CE + text KD)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += accuracy(student_logits_base, labels)\n",
    "\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def train_loop(\n",
    "    teacher_model,\n",
    "    student_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    base_indices,\n",
    "    epochs=10,\n",
    "    alpha=0.5,\n",
    "    beta=0.2,\n",
    "    temperature=2,\n",
    "    model_state_path='adapter_weights.pt'\n",
    "):\n",
    "    best_val_acc = 0.0\n",
    "    history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    # pre-computing CLIP frozen language features such that to avoid re-computing them at each epoch\n",
    "    with torch.no_grad():\n",
    "        dtype = teacher_model.dtype\n",
    "        classnames = student_model.classnames\n",
    "        template = \"a photo of a {}, a type of flower.\"\n",
    "        prompts = [template.format(c) for c in classnames]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(DEVICE)\n",
    "        teacher_text_feats = teacher_model.encode_text(tokenized_prompts)\n",
    "        teacher_text_feats /= teacher_text_feats.norm(dim=-1, keepdim=True).clamp(min=1e-12) # normalize\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch: {epoch}/{epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            teacher_model, student_model, train_loader, optimizer,\n",
    "            base_indices, teacher_text_feats, alpha, beta, temperature\n",
    "        )\n",
    "        val_loss, val_acc = eval(student_model, val_loader)\n",
    "\n",
    "        scheduler.step()\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "        print(f\"[LR] Current learning rate: {current_lr:.6f}\")\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc.: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc.: {val_acc:.4f}\")\n",
    "\n",
    "        # save model    \n",
    "        best_val_acc = val_acc\n",
    "        best_vision_adapter_state = copy.deepcopy(student_model.vision_adapter.state_dict())\n",
    "        best_ctx_state = copy.deepcopy(student_model.text_adapter.state_dict())\n",
    "        torch.save(best_ctx_state, \"ctx_state.pt\")\n",
    "        torch.save(best_vision_adapter_state, \"vision_adapter_state.pt\")\n",
    "        print(f\"‚úÖ New best model saved (val_acc = {best_val_acc:.4f})\")\n",
    "\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40062b4a",
   "metadata": {
    "id": "40062b4a"
   },
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067c01aa",
   "metadata": {
    "id": "067c01aa"
   },
   "source": [
    "### CLIP-Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f81e098",
   "metadata": {
    "id": "4f81e098"
   },
   "outputs": [],
   "source": [
    "# @title Adapters (CoOp and CLIP-Adapter)\n",
    "\n",
    "class VisionAdapter(nn.Module):\n",
    "    def __init__(self, c_in, reduction=4, dropout=0.5):\n",
    "        super(VisionAdapter, self).__init__()\n",
    "\n",
    "        hidden_dim = c_in // reduction\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features=c_in, out_features=hidden_dim, bias=False),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=hidden_dim, out_features=c_in, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)  # residual\n",
    "        return x\n",
    "\n",
    "\n",
    "class TextAdapter(nn.Module): # CoOp text adapter\n",
    "    def __init__(self, classnames, clip_model, n_ctx=16):\n",
    "        super(TextAdapter, self).__init__()\n",
    "        self.classnames = classnames\n",
    "        self.dtype = clip_model.dtype\n",
    "\n",
    "        ctx_dim = clip_model.ln_final.weight.shape[0]  # context dimension\n",
    "        self.ctx = nn.Parameter(torch.empty(n_ctx, ctx_dim))  # tokens to be learned\n",
    "        nn.init.normal_(self.ctx, std=0.02)\n",
    "\n",
    "        prompt_prefix = \" \".join(['X'] * n_ctx)\n",
    "        print(f'Initial context: \"{prompt_prefix}\"')\n",
    "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
    "\n",
    "        temp = 'a photo of a {}, a type of flower.'\n",
    "        prompts = [temp.format(c) for c in classnames]\n",
    "        prompts = [prompt_prefix + \" \" + p for p in prompts]\n",
    "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embeddings = clip_model.token_embedding(tokenized_prompts).type(self.dtype)\n",
    "\n",
    "        # structure is: [BOS] [class tokens ... ] [EOS]\n",
    "        self.register_buffer(\"token_prefix\", embeddings[:, :1, :])  # SOS\n",
    "        self.register_buffer(\"token_suffix\", embeddings[:, 1 + n_ctx :, :])  # class tokens + EOS\n",
    "        self.tokenized_prompts = tokenized_prompts\n",
    "\n",
    "    def forward(self, token_embeddings):\n",
    "        # append the learnable context to the end of the token embeddings\n",
    "        ctx = self.ctx.unsqueeze(0).expand(token_embeddings.shape[0], -1, -1)\n",
    "        return torch.cat([\n",
    "            self.token_prefix,  # BOS\n",
    "            ctx,  # learned context\n",
    "            self.token_suffix,  # class tokens + EOS\n",
    "        ], dim=1).type(self.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a0d3c",
   "metadata": {
    "id": "260a0d3c"
   },
   "source": [
    "### Custom CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b5eb64",
   "metadata": {
    "id": "69b5eb64"
   },
   "outputs": [],
   "source": [
    "# @title CustomCLIP\n",
    "\n",
    "class CustomCLIP(nn.Module): # Student model with both visual and text adapters\n",
    "    def __init__(\n",
    "        self,\n",
    "        classnames: list[str],\n",
    "        clip_model: nn.Module,\n",
    "        vision_adapter_args,\n",
    "        text_adapter_args,\n",
    "        vision_ratio: float = 0.2,\n",
    "        vision_adapter_state=None,\n",
    "        text_adapter_state=None \n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.classnames = classnames\n",
    "        self.dtype = clip_model.dtype\n",
    "        self.clip_model = clip_model\n",
    "        self.vision_ratio = vision_ratio\n",
    "        self.logit_scale = clip_model.logit_scale\n",
    "        self.image_encoder = clip_model.visual\n",
    "\n",
    "        # create and attach adapters\n",
    "        vision_feature_dim = self.image_encoder.output_dim\n",
    "        self.vision_adapter = VisionAdapter(vision_feature_dim, **vision_adapter_args)\n",
    "        self.text_adapter = TextAdapter(classnames, clip_model, **text_adapter_args)\n",
    "\n",
    "        if vision_adapter_state:\n",
    "            self.vision_adapter.load_state_dict(vision_adapter_state)\n",
    "\n",
    "        if text_adapter_state:\n",
    "            self.text_adapter.load_state_dict(text_adapter_state)\n",
    "\n",
    "    def encode_image_with_adapter(self, image):\n",
    "        image_features = self.image_encoder(image.type(self.dtype))\n",
    "        adapted_features = self.vision_adapter(image_features.float()).type(self.dtype)\n",
    "        combined_features = self.vision_ratio * adapted_features + (1 - self.vision_ratio) * image_features\n",
    "        # norm & return\n",
    "        return combined_features / combined_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def encode_text_with_adapter(self):\n",
    "        text_embeddings = self.clip_model.token_embedding(\n",
    "            self.text_adapter.tokenized_prompts).type(self.dtype)\n",
    "        adapted_embeddings = self.text_adapter(text_embeddings)\n",
    "\n",
    "        # pass through CLIP text transformer\n",
    "        x = adapted_embeddings + self.clip_model.positional_embedding.type(self.dtype)\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
    "        x = self.clip_model.transformer(x)\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
    "        x = self.clip_model.ln_final(x).type(self.dtype)\n",
    "\n",
    "        text_features = x[torch.arange(x.shape[0]), self.text_adapter.tokenized_prompts.argmax(dim=-1)] @ self.clip_model.text_projection\n",
    "        return text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    def forward(self, images: torch.Tensor):\n",
    "        # compute (normalized) text & image feats.\n",
    "        image_features = self.encode_image_with_adapter(images.type(self.dtype))\n",
    "        text_features = self.encode_text_with_adapter()\n",
    "        return image_features, text_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1298169e",
   "metadata": {
    "id": "1298169e"
   },
   "source": [
    "# Preparing the Dataset and CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769719d",
   "metadata": {
    "id": "1769719d"
   },
   "outputs": [],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Inspect classes\n",
    "_, _, tmp_test = get_data()\n",
    "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
    "\n",
    "CLASS_NAMES = [\n",
    "    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold',\n",
    "    'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle',\n",
    "    'snapdragon', \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris',\n",
    "    'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily',\n",
    "    'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth',\n",
    "    'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william',\n",
    "    'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    "    'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose',\n",
    "    'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue',\n",
    "    'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion',\n",
    "    'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium',\n",
    "    'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia',\n",
    "    'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy',\n",
    "    'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy',\n",
    "    'gazania', 'azalea', 'water lily', 'rose', 'thorn apple',\n",
    "    'morning glory', 'passion flower', 'lotus lotus', 'toad lily', 'anthurium',\n",
    "    'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose',\n",
    "    'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily',\n",
    "    'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea',\n",
    "    'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower',\n",
    "    'trumpet creeper', 'blackberry lily'\n",
    "]\n",
    "\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
    "clip_model, preprocess = clip.load(\"ViT-B/16\", device=DEVICE)\n",
    "\n",
    "# get the three datasets\n",
    "train_set, val_set, test_set = get_data(transform=preprocess)\n",
    "\n",
    "# split classes into base and novel\n",
    "base_classes, novel_classes = base_novel_categories(train_set)\n",
    "\n",
    "# split the three datasets\n",
    "train_base, _ = split_data(train_set, base_classes)\n",
    "val_base, _ = split_data(val_set, base_classes)\n",
    "test_base, test_novel = split_data(test_set, base_classes)\n",
    "\n",
    "# Create data loaders\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_base,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_base,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g,\n",
    "    worker_init_fn=seed_worker\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd080209",
   "metadata": {
    "id": "cd080209"
   },
   "source": [
    "# Preparing the custom Model + Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52173f",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "2c52173f"
   },
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "lr = 2e-3\n",
    "wd = 1e-2\n",
    "# vision adapter hyperparams.\n",
    "vision_ratio = 0.2  # original paper is 0.2\n",
    "vision_reduction = 16\n",
    "vision_dropout = 0.4\n",
    "vision_adapter_args = {\"reduction\": vision_reduction, \"dropout\": vision_dropout}\n",
    "# CoOp hyperparam\n",
    "n_ctx = 16\n",
    "text_adapter_args = {\"n_ctx\": n_ctx}\n",
    "temperature = 3\n",
    "\n",
    "# loss weights search space\n",
    "alpha_values = [0.1, 0.15, 0.2, 0.4, 0.6, 0.8]\n",
    "beta_values = [0.5, 1, 2, 4, 8]\n",
    "\n",
    "# generate all parameter combinations\n",
    "param_combinations = list(itertools.product(alpha_values, beta_values))\n",
    "\n",
    "print(\"Number of experiments to run: \", len(param_combinations))\n",
    "print(f\"Alpha values: {alpha_values}\")\n",
    "print(f\"Beta values: {beta_values}\")\n",
    "\n",
    "results = {\n",
    "    \"metadata\": {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_config\": {\n",
    "            \"epochs\": epochs,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": wd,\n",
    "            \"vision_ratio\": vision_ratio,\n",
    "            \"vision_reduction\": vision_reduction,\n",
    "            \"vision_dropout\": vision_dropout,\n",
    "            \"n_ctx\": n_ctx,\n",
    "            \"temperature\": temperature,\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"seed\": SEED\n",
    "        },\n",
    "        \"search_space\": {\n",
    "            \"alpha_values\": alpha_values,\n",
    "            \"beta_values\": beta_values\n",
    "        }\n",
    "    },\n",
    "    \"experiments\": []\n",
    "}\n",
    "\n",
    "# define dataloaders for testing\n",
    "test_base_loader = torch.utils.data.DataLoader(\n",
    "    test_base,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g\n",
    ")\n",
    "test_novel_loader = torch.utils.data.DataLoader(\n",
    "    test_novel,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "# defining teacher once (CLIP base)\n",
    "teacher_model = clip_model\n",
    "teacher_model.eval()\n",
    "\n",
    "# freezing teacher params\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for exp_idx, (alpha, beta) in enumerate(param_combinations, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Experiment {exp_idx}/{len(param_combinations)}\")\n",
    "    print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    set_seed(SEED)\n",
    "    \n",
    "    # defining student model\n",
    "    student_model = CustomCLIP(\n",
    "        CLASS_NAMES,\n",
    "        clip_model,\n",
    "        vision_adapter_args=vision_adapter_args,\n",
    "        text_adapter_args=text_adapter_args,\n",
    "        vision_ratio=vision_ratio\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    trainable_params = chain(\n",
    "        student_model.vision_adapter.parameters(),\n",
    "        student_model.text_adapter.parameters(),\n",
    "    )\n",
    "\n",
    "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=wd)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    # train\n",
    "    history = train_loop(\n",
    "        teacher_model, student_model, train_loader, val_loader,\n",
    "        optimizer, scheduler, epochs=epochs,\n",
    "        base_indices=base_classes, alpha=alpha, beta=beta,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # load model and evaluate\n",
    "    vision_adapter_state = torch.load(\"vision_adapter_state.pt\", weights_only=True)\n",
    "    text_adapter_state = torch.load(\"ctx_state.pt\", weights_only=True)\n",
    "    best_model = CustomCLIP(\n",
    "        CLASS_NAMES,\n",
    "        clip_model,\n",
    "        vision_adapter_args,\n",
    "        text_adapter_args,\n",
    "        vision_ratio,\n",
    "        vision_adapter_state=vision_adapter_state,\n",
    "        text_adapter_state=text_adapter_state\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    # Evaluate on test sets\n",
    "    _, base_acc = eval(best_model, test_base_loader)\n",
    "    _, novel_acc = eval(best_model, test_novel_loader)\n",
    "    hm = harmonic_mean(base_acc, novel_acc)\n",
    "\n",
    "    # save results\n",
    "    experiment_result = {\n",
    "        \"exp_id\": exp_idx,\n",
    "        \"hyperparameters\": {\n",
    "            \"alpha\": alpha,\n",
    "            \"beta\": beta\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"base_accuracy\": float(base_acc),\n",
    "            \"novel_accuracy\": float(novel_acc),\n",
    "            \"harmonic_mean\": float(hm),\n",
    "            \"best_val_acc\": float(max(history[\"val_acc\"])),\n",
    "            \"final_train_acc\": float(history[\"train_acc\"][-1]),\n",
    "            \"final_val_acc\": float(history[\"val_acc\"][-1]),\n",
    "            \"epochs_trained\": len(history[\"train_loss\"])\n",
    "        },\n",
    "        \"history\": {\n",
    "            \"train_loss\": [float(x) for x in history[\"train_loss\"]],\n",
    "            \"train_acc\": [float(x) for x in history[\"train_acc\"]],\n",
    "            \"val_loss\": [float(x) for x in history[\"val_loss\"]],\n",
    "            \"val_acc\": [float(x) for x in history[\"val_acc\"]]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results[\"experiments\"].append(experiment_result)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\nüìä Results:\")\n",
    "    print(f\"  Base Accuracy: {base_acc*100:.2f}%\")\n",
    "    print(f\"  Novel Accuracy: {novel_acc*100:.2f}%\")\n",
    "    print(f\"  Harmonic Mean: {hm*100:.2f}%\")\n",
    "    \n",
    "    # Save intermediate results (in case of interruption)\n",
    "    with open(f\"hyperparam_study_results_intermediate.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "# Save final results\n",
    "output_filename = f\"hyperparam_study_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_filename, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ Study complete! Results saved to: {output_filename}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Print top 5 configurations\n",
    "print(\"\\nüèÜ Top 5 Configurations (by Harmonic Mean):\")\n",
    "sorted_results = sorted(results[\"experiments\"], \n",
    "                       key=lambda x: x[\"metrics\"][\"harmonic_mean\"], \n",
    "                       reverse=True)\n",
    "\n",
    "for i, exp in enumerate(sorted_results[:5], 1):\n",
    "    print(f\"\\n{i}. Alpha: {exp['hyperparameters']['alpha']}, \"\n",
    "          f\"Beta: {exp['hyperparameters']['beta']}\")\n",
    "    print(f\"   Base: {exp['metrics']['base_accuracy']*100:.2f}%, \"\n",
    "          f\"Novel: {exp['metrics']['novel_accuracy']*100:.2f}%, \"\n",
    "          f\"HM: {exp['metrics']['harmonic_mean']*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
